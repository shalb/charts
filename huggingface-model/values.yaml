## Huggingface model
## @section Model
## ref: https://huggingface.co/models
## @param model.organization Models' company name on huggingface, required!
## @param model.name Models' name on huggingface, required!
## @param model.hf_user Huggingface git user, optional
## @param model.hf_token Huggingface git token, optional
## e.g. to deploy model https://huggingface.co/HuggingFaceH4/zephyr-7b-beta use configuration below:
## organization: HuggingFaceH4
## name: zephyr-7b-beta
##
model:
  organization: "mycompany"
  name: "demo"
  hf_user: ""
  hf_token: ""

## Init configuration. By default, init clone model from Huggingface git using git-lfs.
## The another way is to upload model to s3 bucket to reduce init delay and external traffic.
## @param init.s3.enabled Turn on/off s3 data source Default: disabled
## @param init.s3.bucketURL Full s3 URL included path to model's folder
##
init:
  s3:
    enabled: false
    bucketURL: s3://k8s-model-zephyr/llm/deployment/HuggingFaceH4/zephyr-7b-beta

## Huggingface block configure running text-generation-launcher internal port and additional arguments
## @param huggingface.containerPort Deployment/StatefulSet ContainerPort, optional
##
huggingface:
  containerPort: 8080
  ## @param huggingface.args Additional arg for text-generation-launcher optional
  ## e.g.
  ##  args:
  ##   - "--quantize"
  ##   - "bitsandbytes"
  ##   - "--num-shard"
  ##   - "1"
  ##
  args: []

## @section Global
## @param replicaCount Deployment/StatefulSet replicaCount
##
replicaCount: 1

## @param kind Resource king [allowed values: Deployment/StatefulSet, optional]
##
kind: Deployment

## Huggingface image
## @param image.repo Huggingface image repo
## @param image.tag Huggingface image version
## @param image.pullPolicy Huggingface image pull policy
## ref: https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
##
image:
  repo: ghcr.io/huggingface/text-generation-inference
  tag: "latest"
  pullPolicy: IfNotPresent

## @param imagePullSecrets May need if used private repo as a cache for image ghcr.io/huggingface/text-generation-inference
##
imagePullSecrets: []

## @param nameOverride String to partially override common.names.name
##
nameOverride: ""

## @param fullnameOverride String to fully override common.names.fullname
##
fullnameOverride: ""

## Start model pod(s) without limitations on shm memory.
## By default docker and containerd (and possibly other container runtimes) limit `/dev/shm` to `64M`
## ref: https://github.com/containerd/containerd/issues/3654
##
shmVolume:
  ## @param shmVolume.enabled Enable emptyDir volume for /dev/shm for model pod(s)
  ##
  enabled: true
  ## @param shmVolume.sizeLimit Set this to enable a size limit on the shm tmpfs
  ## Note: the size of the tmpfs counts against container's memory limit
  ##
  sizeLimit: "1Gi"

## Persistence parameters
## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
## @param persistence.accessModes PVC accessModes
## @param persistence.storageClassName Kubernetes storageClass name
## @param persistence.storage Volume size
##
persistence:
  accessModes:
  - ReadWriteOnce
  storageClassName: gp2
  storage: 100Gi

## Service parameters
## ref: https://kubernetes.io/docs/concepts/services-networking/
## @param service.port Service port, default 8080
## @param service.type Service type, default ClusterIP
##
service:
  port: 8080
  type: "ClusterIP"

## ServiceAccount parameters
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
## @param serviceAccount.create Enable/disable service account, default enabled
## @param serviceAccount.role Kubernetes role configuration, default nil
##
serviceAccount:
  create: true
  role: {}
#     rules:
#     - apiGroups:
#         - ""
#       resources:
#         - endpoints
#         - pods
#         - nodes
#         - services
#       verbs:
#         - get
#         - list

## @param podAnnotations Annotations for replicas pods
## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
##
podAnnotations: {}

## @param updateStrategy specifies the strategy used to replace old Pods by new ones. Can be set to `RollingUpdate`, `OnDelete` (StatefulSet), `Recreate` (Deployment)
## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
updateStrategy: {}
## e.g. (rollingUpdate option available Only for Deployment)
#   type: RollingUpdate
#   rollingUpdate:
#     maxUnavailable: 1

## Configure Pods Security Context
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
## @param securityContext Set pod's Security Context fsGroup
##
securityContext: {}
#   capabilities:
#     drop:
#     - ALL
#   readOnlyRootFilesystem: true
#   runAsNonRoot: true
#   runAsUser: 1000

## @param extraEnvVars Array with extra environment variables to add to main pod
## e.g:
## extraEnvVars:
##   - name: FOO
##     value: "bar"
##
extraEnvVars: []

## Configure the ingress resource that allows you to access the model API
## @param ingress.enabled Enable/disable ingress(es) for model API, default disabled
##
ingress:
  enabled: false
  ## ingress configuration
  ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
  ## e.g.
  #annotations:
  #  cert-manager.io/cluster-issuer: "letsencrypt-http"
  #hosts:
  #  - host: api.model.example.com
  #    paths:
  #      - path: /
  #        pathType: Prefix
  #tls:
  #  - hosts:
  #      - api.model.example.com
  #    secretName: huggingface-model


## @param livenessProbe Configure extra options for model liveness probe
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
##
livenessProbe: {}
#   failureThreshold: 4
#   httpGet:
#     path: /
#   initialDelaySeconds: 1
#   periodSeconds: 5
#   successThreshold: 1
#   timeoutSeconds: 3

## @param readinessProbe Configure extra options for model readiness probe
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
##
readinessProbe: {}
#   failureThreshold: 3
#   httpGet:
#     path: /
#   initialDelaySeconds: 1
#   periodSeconds: 3
#   successThreshold: 2
#   timeoutSeconds: 2

## @param startupProbe Configure extra options for model startup probe
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
##
startupProbe: {}
#   failureThreshold: 10
#   httpGet:
#     path: /
#   initialDelaySeconds: 15
#   periodSeconds: 5
#   successThreshold: 1
#   timeoutSeconds: 3

## PodDisruptionBudget configuration
pdb:
  ## @param pdb.create Specifies whether a PodDisruptionBudget should be created
  ##
  create: false
  ## @param pdb.minAvailable Min number of pods that must still be available after the eviction
  ##
  minAvailable: 1
  ## @param pdb.maxUnavailable Max number of pods that can be unavailable after the eviction
  ##
  maxUnavailable: ""

## Model container's resource requests and limits
## ref: https://kubernetes.io/docs/user-guide/compute-resources/
## @param resources.limits.nvidia.com/gpu The required option by text-generation-launcher
## @param resources.requests.cpu The requested CPU minimal recommended value
## @param resources.requests.memory The requested memory minimal recommended size
##
resources:
  requests:
    cpu: "3"
    memory: "10Gi"
  limits:
    nvidia.com/gpu: 1

## @param extraVolumes Optionally specify extra list of additional volumes for models' pods
## e.g.
##   - hostPath:
##       path: /opt/model/logs
##       type: DirectoryOrCreate
##     name: logging
##
extraVolumes: []

## @param extraVolumeMounts Optionally specify extra list of additional volumeMounts for models' container
## e.g.
##   - mountPath: /opt/model/logs
##     name: logging
##
extraVolumeMounts: []

## Model Autoscaling configuration
## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
## @param autoscaling.enabled Enable Horizontal POD autoscaling for model
## @param autoscaling.minReplicas Minimum number of model replicas
## @param autoscaling.maxReplicas Maximum number of model replicas
## @param autoscaling.targetCPU Target CPU utilization percentage
## @param autoscaling.targetMemory Target Memory utilization percentage
##
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 5
  targetCPU: 50
  targetMemory: 50

## @param affinity Affinity for pod assignment
## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
## NOTE: podAffinityPreset, podAntiAffinityPreset, and nodeAffinityPreset will be ignored when it's set
##
affinity: {}
## @param nodeSelector Node labels for pod assignment
## ref: https://kubernetes.io/docs/user-guide/node-selection/
##
nodeSelector: {}
## @param tolerations Tolerations for pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
##
tolerations: []

## @section Chat
## Chat-UI used to access the model in ChatGPT style
## ref: https://github.com/huggingface/chat-ui
chat:
  ## @param chat.enabled Enables chat-ui for the model installed with the chart.
  enabled: false

  ## @param chat.replicaCount Number of chat replicas
  replicaCount: 1

  ## @param chat.extraEnvVars Extra environment variables
  ## ref: https://github.com/huggingface/chat-ui#extra-parameters
  extraEnvVars: []
  ## e.g.
  # - name: PUBLIC_ORIGIN
  #   value: "http://localhost:8080"

  ## @param chat.modelConfig Configuration for the chat model
  ## ref: https://github.com/huggingface/chat-ui#custom-models
  modelConfig: {}
    ## e.g.
    # parameters:
    #   temperature: 0.1
    #   top_p: 0.95
    #   repetition_penalty: 1.2
    #   top_k: 50
    #   truncate: 1000
    #   max_new_tokens: 1024
    # datasetName: OpenAssistant/oasst1
    # description: A good alternative to ChatGPT
    # websiteUrl: https://open-assistant.io
    # userMessageToken: ""
    # assistantMessageToken: ""
    # messageEndToken: "</s>"
    # preprompt: |
    #   Below are a series of dialogues between various people and an AI assistant. The AI tries to be helpful, polite, honest, sophisticated, emotionally aware, and humble-but-knowledgeable. The assistant is happy to help with almost anything, and will do its best to understand exactly what is needed. It also tries to avoid giving false or misleading information, and it caveats when it isn't entirely sure about the right answer. That said, the assistant is practical and really does its best, and doesn't let caution get too much in the way of being useful.
    #   -----
    # promptExamples:
    # - title: Write an email from bullet list
    #   prompt: "As a restaurant owner, write a professional email to the supplier to
    #     get these products every week: \n\n- Wine (x10)\n- Eggs (x24)\n- Bread (x12)"
    # - title: Code a snake game
    #   prompt: Code a basic snake game in python, give explanations for each step.
    # - title: Assist in a task
    #   prompt: How do I make a delicious lemon cheesecake?
    # parameters:
    #   temperature: 0.9
    #   top_p: 0.95
    #   repetition_penalty: 1.2
    #   top_k: 50

  ## @param chat.additionalModels Additional models for the chat
  additionalModels: []
    ## e.g.
    # - name: "Llama-2-70b-chat-hf
    #   endpoints:
    #     - url: "http://exampl.com:8080/model/api"
    #       type: "tgi"
    #   parameters:
    #     temperature: 0.1
    #     top_p: 0.95
    #     repetition_penalty: 1.2
    #     top_k: 50
    #     truncate: 1000
    #     max_new_tokens: 1024
    #   datasetName: OpenAssistant/oasst1

  ## @param chat.podAnnotations Annotations for chat pods
  podAnnotations: {}

  ## @param chat.imagePullSecrets Secrets for image pulling
  imagePullSecrets: []

  ## @param chat.affinity Affinity for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## NOTE: podAffinityPreset, podAntiAffinityPreset, and nodeAffinityPreset will be ignored when it's set
  affinity: {}

  ## @param chat.nodeSelector Node labels for pod assignment
  ## ref: https://kubernetes.io/docs/user-guide/node-selection/
  nodeSelector: {}

  ## @param chat.tolerations Tolerations for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  ## @param chat.resources.requests.cpu The requested CPU minimal recommended value
  ## @param chat.resources.requests.memory The requested memory minimal recommended size
  ##
  resources:
    requests:
      cpu: "0.5"
      memory: "512M"

  ## Chat-UI Image configuration
  ## @param chat.image.repo chat-ui image repo(we have modified Dockerfile to accept env variables)
  ## ref: https://github.com/voatsap/chat-ui/blob/main/Dockerfile
  ## @param chat.image.tag Huggingface image version
  ## @param chat.image.pullPolicy Huggingface image pull policy
  ## ref: https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
  ##
  image:
    repo: "shalb/hf-chat-ui"
    tag: "v0.8"
    pullPolicy: IfNotPresent

  ## @param chat.mongodb MongoDB configuration for the chat, if mongodb.install is set to true,
  ## it would be auto configured for the mongodb installed with the chart
  mongodb: {}
    ## e.g.
    #host: ""
    #user: "root"
    #password: ""
    #port: "27017"
    #urlParams: "admin?directConnection=true&authSource=admin"

  ## @param chat.ingress.enabled Ingress configuration for the chat service
  ingress:
    enabled: false
    ## e.g.
    ## Ingress annotations
    #annotations:
    #  cert-manager.io/cluster-issuer: "letsencrypt-prod"
    ## Hosts and paths for Ingress
    #hosts:
    #  - host: api.model.example.com
    #    paths:
    #      - path: /
    #        pathType: Prefix
    ## TLS configuration for Ingress
    #tls:
    #  - hosts:
    #      - api.model.example.com
    #    secretName: huggingface-model

## @section MongoDB
## @param mongodb.install Install MongoDB
## @param mongodb.auth.rootPassword Root password for MongoDB
mongodb:
  install: true
  auth:
    rootPassword: ""
